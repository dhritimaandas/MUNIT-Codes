{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from load_data import loading_data\n",
    "# from net_structure_img import img_net_strucuture\n",
    "# from net_structure_txt import txt_net_strucuture\n",
    "# from utils.calc_hammingranking import calc_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environmental setting: setting the following parameters based on your experimental environment.\n",
    "select_gpu = '0'\n",
    "per_process_gpu_memory_fraction = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data parameters\n",
    "#DATA_DIR = 'data/FLICKR-25K.mat'\n",
    "# TRAINING_SIZE = 10000\n",
    "# QUERY_SIZE = 2000\n",
    "# DATABASE_SIZE = 18015\n",
    "# DATA_DIR = '/home/iacvlab/Titir/Work_3/Sketchy_extended_features/' # ---- local\n",
    "DATA_DIR = '/home/titir/work_sbir/ft_vgg_feat/'   # ------------ cluster\n",
    "train_=80\n",
    "val_=10\n",
    "te_=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "MAX_ITER = 1\n",
    "gamma = 1\n",
    "eta = 1\n",
    "bit = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'log/result_' + datetime.now().strftime(\"%d-%h-%m-%s\") + '_' + str(bit) + 'bits_Sketchy_extended.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_img_net(image_input, cur_f_batch, var, ph, train_x, train_L, lr, train_step_x, mean_pixel_, Sim):\n",
    "    F = var['F']\n",
    "    batch_size = var['batch_size']\n",
    "    num_train = train_x.shape[0]\n",
    "    # index = range(0, num_train - 1, 1)\n",
    "    for iter in xrange(num_train / batch_size):\n",
    "        index = np.random.permutation(num_train)\n",
    "        ind = index[0: batch_size]\n",
    "        # ind = index[iter * batch_size: (iter + 1) * batch_size]\n",
    "        unupdated_ind = np.setdiff1d(range(num_train), ind)\n",
    "        sample_L = train_L[ind, :]\n",
    "        image = train_x[ind, :, :, :].astype(np.float64)\n",
    "        image = image - mean_pixel_.astype(np.float64)\n",
    "\n",
    "        S = calc_neighbor(sample_L, train_L)\n",
    "        cur_f = cur_f_batch.eval(feed_dict={image_input: image})\n",
    "        F[:, ind] = cur_f\n",
    "\n",
    "        train_step_x.run(feed_dict={ph['S_x']: S, ph['G']: var['G'], ph['b_batch']: var['B'][:, ind],\n",
    "                                                                ph['F_']: F[:, unupdated_ind], ph['lr']: lr, image_input: image})\n",
    "\n",
    "    return F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_txt_net(text_input, cur_g_batch, var, ph, train_y, train_L, lr, train_step_y, Sim):\n",
    "    G = var['G']\n",
    "    batch_size = var['batch_size']\n",
    "    num_train = train_x.shape[0]\n",
    "    for iter in xrange(num_train / batch_size):\n",
    "        index = np.random.permutation(num_train)\n",
    "        ind = index[0: batch_size]\n",
    "        unupdated_ind = np.setdiff1d(range(num_train), ind)\n",
    "        sample_L = train_L[ind, :]\n",
    "        text = train_y[ind, :].astype(np.float32)\n",
    "        text = text.reshape([text.shape[0],1,text.shape[1],1])\n",
    "        S = calc_neighbor(train_L, sample_L)\n",
    "        cur_g = cur_g_batch.eval(feed_dict={text_input: text})\n",
    "        G[:, ind] = cur_g\n",
    "\n",
    "        train_step_y.run(feed_dict={ph['S_y']: S, ph['F']: var['F'], ph['b_batch']: var['B'][:, ind],\n",
    "                                                                ph['G_']: G[:, unupdated_ind], ph['lr']: lr, text_input: text})\n",
    "    return G\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calc_neighbor(label_1, label_2):\n",
    "    Sim = (np.dot(label_1, label_2.transpose()) > 0).astype(int)\n",
    "    return Sim\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calc_loss(B, F, G, Sim, gamma, eta):\n",
    "    theta = np.matmul(np.transpose(F), G) / 2\n",
    "    term1 = np.sum(np.log(1+np.exp(theta)) - Sim * theta)\n",
    "    term2 = np.sum(np.power((B-F), 2) + np.power(B-G,2))\n",
    "    term3 = np.sum(np.power(np.matmul(F, np.ones((F.shape[1],1))),2)) + np.sum(np.power(np.matmul(G, np.ones((F.shape[1],1))),2))\n",
    "    loss = term1 + gamma * term2 + eta * term3\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_image_code(image_input, cur_f_batch, X, bit, mean_pixel):\n",
    "    batch_size = 128\n",
    "    num_data = X.shape[0]\n",
    "    index = np.linspace(0, num_data - 1, num_data).astype(int)\n",
    "    B = np.zeros([num_data, bit], dtype=np.float32)\n",
    "    for iter in xrange(num_data / batch_size + 1):\n",
    "        ind = index[iter * batch_size : min((iter + 1)*batch_size, num_data)]\n",
    "        mean_pixel_ = np.repeat(mean_pixel[0][2][:, :, :, np.newaxis], len(ind), axis=3)\n",
    "        image = X[ind, :, :, :].astype(np.float32) - mean_pixel_.astype(np.float32).transpose(3, 0, 1, 2)\n",
    "        cur_f = cur_f_batch.eval(feed_dict={image_input: image})\n",
    "        B[ind, :] = cur_f.transpose()\n",
    "    B = np.sign(B)\n",
    "        #pdb.set_trace()\n",
    "    return B\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_text_code(text_input, cur_g_batch, Y, bit):\n",
    "    batch_size = 128\n",
    "    num_data = Y.shape[0]\n",
    "    index = np.linspace(0, num_data - 1, num_data).astype(int)\n",
    "    B = np.zeros([num_data, bit], dtype=np.float32)\n",
    "    for iter in xrange(num_data / batch_size + 1):\n",
    "                #pdb.set_trace()\n",
    "        ind = index[iter * batch_size : min((iter + 1)*batch_size, num_data)]\n",
    "        text = Y[ind, :].astype(np.float32)\n",
    "        text = text.reshape([text.shape[0],1,text.shape[1],1])\n",
    "        cur_g = cur_g_batch.eval(feed_dict={text_input: text})\n",
    "        B[ind, :] = cur_g.transpose()\n",
    "    B = np.sign(B)\n",
    "        #pdb.set_trace()\n",
    "    return B\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_validation(B, query_L, train_L, qBX, qBY):\n",
    "    mapi2t = calc_map(qBX, B, query_L, train_L)\n",
    "    mapt2i = calc_map(qBY, B, query_L, train_L)\n",
    "    return mapi2t, mapt2i\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "LAYER1_NODE = 8192\n",
    "\n",
    "def txt_net_strucuture(text_input, dimy, bit):\n",
    "\n",
    "    W_fc8 = tf.random_normal([1, dimy, 1, LAYER1_NODE], stddev=1.0) * 0.01\n",
    "    b_fc8 = tf.random_normal([1, LAYER1_NODE], stddev=1.0) * 0.01\n",
    "    fc1W = tf.Variable(W_fc8)\n",
    "    fc1b = tf.Variable(b_fc8)\n",
    "\n",
    "    # ### debugging .......................\n",
    "    # wb = scipy.io.loadmat('data/wb-text.mat')\n",
    "    # fc1W = tf.Variable(wb['w1'] * 0.01)\n",
    "    # fc1b = tf.Variable(wb['b1'] * 0.01)\n",
    "\n",
    "    conv1 = tf.nn.conv2d(text_input, fc1W, strides=[1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "    layer1 = tf.nn.relu(tf.nn.bias_add(conv1, tf.squeeze(fc1b)))\n",
    "\n",
    "    W_fc2 = tf.random_normal([1, 1, LAYER1_NODE, bit], stddev=1.0) * 0.01\n",
    "    b_fc2 = tf.random_normal([1, bit], stddev=1.0) * 0.01\n",
    "    fc2W = tf.Variable(W_fc2)\n",
    "    fc2b = tf.Variable(b_fc2)\n",
    "\n",
    "    # ### debugging .......................\n",
    "    # fc2W = tf.Variable(wb['w2'] * 0.01)\n",
    "    # fc2b = tf.Variable(wb['b2'] * 0.01)\n",
    "    conv2 = tf.nn.conv2d(layer1, fc2W, strides=[1, 1, 1, 1], padding='VALID')\n",
    "    output_g = tf.transpose(tf.squeeze(tf.nn.bias_add(conv2, tf.squeeze(fc2b))))\n",
    "    return output_g\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "LAYER1_NODE = 8192\n",
    "\n",
    "def img_net_strucuture(img_input, dimx, bit):\n",
    "\n",
    "    W_fc8 = tf.random_normal([1, dimx, 1, LAYER1_NODE], stddev=1.0) * 0.01\n",
    "    b_fc8 = tf.random_normal([1, LAYER1_NODE], stddev=1.0) * 0.01\n",
    "    fc1W = tf.Variable(W_fc8)\n",
    "    fc1b = tf.Variable(b_fc8)\n",
    "\n",
    "    # ### debugging .......................\n",
    "    # wb = scipy.io.loadmat('data/wb-text.mat')\n",
    "    # fc1W = tf.Variable(wb['w1'] * 0.01)\n",
    "    # fc1b = tf.Variable(wb['b1'] * 0.01)\n",
    "\n",
    "    conv1 = tf.nn.conv2d(img_input, fc1W, strides=[1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "    layer1 = tf.nn.relu(tf.nn.bias_add(conv1, tf.squeeze(fc1b)))\n",
    "\n",
    "    W_fc2 = tf.random_normal([1, 1, LAYER1_NODE, bit], stddev=1.0) * 0.01\n",
    "    b_fc2 = tf.random_normal([1, bit], stddev=1.0) * 0.01\n",
    "    fc2W = tf.Variable(W_fc2)\n",
    "    fc2b = tf.Variable(b_fc2)\n",
    "\n",
    "    # ### debugging .......................\n",
    "    # fc2W = tf.Variable(wb['w2'] * 0.01)\n",
    "    # fc2b = tf.Variable(wb['b2'] * 0.01)\n",
    "    conv2 = tf.nn.conv2d(layer1, fc2W, strides=[1, 1, 1, 1], padding='VALID')\n",
    "    output_g = tf.transpose(tf.squeeze(tf.nn.bias_add(conv2, tf.squeeze(fc2b))))\n",
    "    return output_g\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_data(path):\n",
    "    img_file = sio.loadmat(os.path.join(path,'feat_Sketchy_extended_img.mat'))\n",
    "    sk_file = sio.loadmat(os.path.join(path,'feat_Sketchy_extended_sk.mat'))\n",
    "    \n",
    "    images = img_file['img_feat']\n",
    "    sketches = sk_file['sk_feat']\n",
    "    \n",
    "    img_labels = img_file['img_label'].reshape(images.shape[0],)\n",
    "    sk_labels = sk_file['sk_label'].reshape(sketches.shape[0],)\n",
    "    \n",
    "    return images, img_labels, sketches, sk_labels\n",
    "\n",
    "\n",
    "\n",
    "def arrange_split_idx(img_label):\n",
    "    \n",
    "    unique_img_label=np.unique(img_label)\n",
    "    num_unique_img_label=unique_img_label.shape[0]\n",
    "    \n",
    "    train_idx=np.empty((0,))\n",
    "    val_idx=np.empty((0,))\n",
    "    te_idx=np.empty((0,))\n",
    "\n",
    "    for i in range(num_unique_img_label):\n",
    "        select_label=unique_img_label[i]\n",
    "        select_label_idx=np.where(select_label==img_label)\n",
    "        num_select_label_samples=select_label_idx[0].shape[0]\n",
    "\n",
    "        num_train=np.ceil(train_*num_select_label_samples/100).astype(int)\n",
    "        select_train_label_idx=select_label_idx[0][:num_train]\n",
    "\n",
    "        no_train_label_idx=np.setdiff1d(select_label_idx,select_train_label_idx)\n",
    "        num_no_train_sample=no_train_label_idx.shape[0]\n",
    "\n",
    "        num_val=np.ceil(val_*num_select_label_samples/100).astype(int)\n",
    "        select_val_label_idx=no_train_label_idx[:num_val]\n",
    "\n",
    "        select_te_label_idx=np.setdiff1d(no_train_label_idx,select_val_label_idx)\n",
    "        \n",
    "        train_idx=np.append(train_idx,select_train_label_idx,axis=0).astype(int)\n",
    "        val_idx=np.append(val_idx,select_val_label_idx,axis=0).astype(int)\n",
    "        te_idx=np.append(te_idx,select_te_label_idx,axis=0).astype(int)\n",
    "        \n",
    "        \n",
    "    return train_idx, val_idx, te_idx\n",
    "\n",
    "\n",
    "\n",
    "def get_one_hot(targets, nb_classes):\n",
    "    targets=targets.astype(int)\n",
    "    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "    return res.reshape(list(targets.shape)+[nb_classes])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def arrange_data(X_train,l_X_train,Y_train,l_Y_train):\n",
    "    \n",
    "    unique_l_X=np.unique(l_X_train)\n",
    "    unique_l_Y=np.unique(l_Y_train)\n",
    "    \n",
    "    x_dim=X_train.shape[1]\n",
    "    y_dim=Y_train.shape[1]\n",
    "    \n",
    "    X_selected_train=np.empty((0,x_dim))\n",
    "    Y_selected_train=np.empty((0,y_dim))\n",
    "    L_selected_train=np.empty((0,))\n",
    "    \n",
    "    for i in range(unique_l_X.shape[0]):\n",
    "        l_X=unique_l_X[i]\n",
    "        \n",
    "        X_idx=np.where(l_X==l_X_train)\n",
    "        Y_idx=np.where(l_X==l_Y_train)\n",
    "        \n",
    "        num_X_idx=X_idx[0].shape[0]\n",
    "        num_Y_idx=Y_idx[0].shape[0]\n",
    "        \n",
    "        num_sample=np.minimum(num_X_idx,num_Y_idx)\n",
    "        \n",
    "        X_selected_train=np.append(X_selected_train,X_train[X_idx[0][0:num_sample],:],axis=0)\n",
    "        Y_selected_train=np.append(Y_selected_train,Y_train[Y_idx[0][0:num_sample],:],axis=0)\n",
    "        L_selected_train=np.append(L_selected_train,l_X_train[X_idx[0][0:num_sample]],axis=0)\n",
    "        \n",
    "    \n",
    "    return X_selected_train,Y_selected_train,L_selected_train\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def split_data(images, tags, labels):\n",
    "    X = {}\n",
    "    X['query'] = images[0:QUERY_SIZE,:,:,:]\n",
    "    X['train'] = images[QUERY_SIZE:TRAINING_SIZE + QUERY_SIZE,:,:,:]\n",
    "    X['retrieval'] = images[QUERY_SIZE:DATABASE_SIZE + QUERY_SIZE,:,:,:]\n",
    "\n",
    "    Y = {}\n",
    "    Y['query'] = tags[0:QUERY_SIZE,:]\n",
    "    Y['train'] = tags[QUERY_SIZE:TRAINING_SIZE + QUERY_SIZE,:]\n",
    "    Y['retrieval'] = tags[QUERY_SIZE:DATABASE_SIZE + QUERY_SIZE,:]\n",
    "\n",
    "    L = {}\n",
    "    L['query'] = labels[0:QUERY_SIZE,:]\n",
    "    L['train'] = labels[QUERY_SIZE:TRAINING_SIZE + QUERY_SIZE,:]\n",
    "    L['retrieval'] = labels[QUERY_SIZE:DATABASE_SIZE + QUERY_SIZE,:]\n",
    "\n",
    "    return X, Y, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, img_labels, sketches, sk_labels = loading_data(DATA_DIR)\n",
    "\n",
    "num_classes = np.unique(img_labels).shape[0]\n",
    "\n",
    "img_train_idx, img_val_idx,img_te_idx=arrange_split_idx(img_labels)\n",
    "\n",
    "img_one_hot=get_one_hot(img_labels,num_classes)\n",
    "\n",
    "X_train=images[img_train_idx,:]\n",
    "l_X_train=img_labels[img_train_idx]\n",
    "\n",
    "\n",
    "X_val=images[img_val_idx,:]\n",
    "l_X_val=img_labels[img_val_idx]\n",
    "\n",
    "X_te=images[img_te_idx,:]\n",
    "l_X_te=img_labels[img_te_idx]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sk_train_idx, sk_val_idx, sk_te_idx=arrange_split_idx(sk_labels)\n",
    "\n",
    "num_classes = np.unique(sk_labels).shape[0]\n",
    "\n",
    "sk_one_hot=get_one_hot(sk_labels,num_classes)\n",
    "\n",
    "\n",
    "Y_train=sketches[img_train_idx,:]\n",
    "l_Y_train=sk_labels[img_train_idx]\n",
    "\n",
    "Y_val=sketches[img_val_idx,:]\n",
    "l_Y_val=sk_labels[img_val_idx]\n",
    "\n",
    "Y_te=sketches[img_te_idx,:]\n",
    "l_Y_te=sk_labels[img_te_idx]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_selected_train,Y_selected_train,L_selected_train=arrange_data(X_train,l_X_train,Y_train,l_Y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X = {}\n",
    "X['query'] = X_te\n",
    "X['train'] = X_selected_train\n",
    "X['retrieval'] = X_te\n",
    "\n",
    "Y = {}\n",
    "Y['query'] = Y_te\n",
    "Y['train'] = Y_selected_train\n",
    "Y['retrieval'] = Y_te\n",
    "\n",
    "L = {}\n",
    "L['query'] = get_one_hot(l_X_te,num_classes)\n",
    "L['train'] = get_one_hot(L_selected_train,num_classes)\n",
    "L['retrieval'] = get_one_hot(l_Y_te,num_classes)\n",
    "\n",
    "\n",
    "\n",
    "print('...loading and splitting data finish')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     images, tags, labels = loading_data(DATA_DIR)\n",
    "#     ydim = tags.shape[1]\n",
    "\n",
    "#     X, Y, L = split_data(images, tags, labels)\n",
    "#     print('...loading and splitting data finish')\n",
    "    \n",
    "gpuconfig = tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=per_process_gpu_memory_fraction))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = select_gpu\n",
    "\n",
    "\n",
    "xdim=X_selected_train.shape[1]\n",
    "ydim=Y_selected_train.shape[1]\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session(config=gpuconfig) as sess:\n",
    "    \n",
    "#     # construct image network\n",
    "#     image_input = tf.placeholder(tf.float32, (None,) + (224, 224, 3))\n",
    "#     net, _meanpix = img_net_strucuture(image_input, bit)\n",
    "#             # pdb.set_trace()\n",
    "#     mean_pixel_ = np.repeat(_meanpix[0][2][:, :, :, np.newaxis], batch_size, axis=3).transpose(3,0,1,2)\n",
    "#     cur_f_batch = tf.transpose(net['fc8'])\n",
    "    \n",
    "    \n",
    "    # construct image network\n",
    "    image_input = tf.placeholder(tf.float32, (None,) + (1, xdim, 1))\n",
    "    cur_f_batch = img_net_strucuture(image_input, xdim, bit)\n",
    "    \n",
    "    \n",
    "    # construct text network\n",
    "    text_input = tf.placeholder(tf.float32, (None,) + (1, ydim, 1))\n",
    "    cur_g_batch = txt_net_strucuture(text_input, ydim, bit)\n",
    "\n",
    "    # training DCMH algorithm\n",
    "    train_L = L['train']\n",
    "    train_x = X['train']\n",
    "    train_y = Y['train']\n",
    "\n",
    "    query_L = L['query']\n",
    "    query_x = X['query']\n",
    "    query_y = Y['query']\n",
    "\n",
    "    retrieval_L = L['retrieval']\n",
    "    retrieval_x = X['retrieval']\n",
    "    retrieval_y = Y['retrieval']\n",
    "    num_train = train_x.shape[0]\n",
    "\n",
    "    Sim = calc_neighbor(train_L, train_L)\n",
    "\n",
    "    var = {}\n",
    "    # lr = np.logspace(-1.5, -3, MAX_ITER)\n",
    "    lr = np.linspace(np.power(10, -1.5), np.power(10, -6.), MAX_ITER)\n",
    "\n",
    "    var['lr'] = lr\n",
    "    var['batch_size'] = batch_size\n",
    "\n",
    "    var['F'] = np.random.randn(bit, num_train)\n",
    "    var['G'] = np.random.randn(bit, num_train)\n",
    "    var['B'] = np.sign(var['F']+var['G'])\n",
    "\n",
    "    unupdated_size = num_train - batch_size\n",
    "    var['unupdated_size'] = unupdated_size\n",
    "\n",
    "    ph = {}\n",
    "    ph['lr'] = tf.placeholder('float32', (), name='lr')\n",
    "    ph['S_x'] = tf.placeholder('float32', [batch_size, num_train], name='pS_x')\n",
    "    ph['S_y'] = tf.placeholder('float32', [num_train, batch_size], name='pS_y')\n",
    "    ph['F'] = tf.placeholder('float32', [bit, num_train], name='pF')\n",
    "    ph['G'] = tf.placeholder('float32', [bit, num_train], name='pG')\n",
    "    ph['F_'] = tf.placeholder('float32', [bit, unupdated_size], name='unupdated_F')\n",
    "    ph['G_'] = tf.placeholder('float32', [bit, unupdated_size], name='unupdated_G')\n",
    "    ph['b_batch'] = tf.placeholder('float32', [bit, batch_size], name='b_batch')\n",
    "    ph['ones_'] = tf.constant(np.ones([unupdated_size, 1], 'float32'))\n",
    "    ph['ones_batch'] = tf.constant(np.ones([batch_size, 1], 'float32'))\n",
    "\n",
    "    theta_x = 1.0 / 2 * tf.matmul(tf.transpose(cur_f_batch), ph['G'])\n",
    "    theta_y = 1.0 / 2 * tf.matmul(tf.transpose(ph['F']), cur_g_batch)\n",
    "\n",
    "    logloss_x = -tf.reduce_sum(tf.multiply(ph['S_x'], theta_x) - tf.log(1.0 + tf.exp(theta_x)))\n",
    "    quantization_x = tf.reduce_sum(tf.pow((ph['b_batch'] - cur_f_batch), 2))\n",
    "    balance_x = tf.reduce_sum(tf.pow(tf.matmul(cur_f_batch, ph['ones_batch']) + tf.matmul(ph['F_'], ph['ones_']), 2))\n",
    "    loss_x = tf.div(logloss_x + gamma * quantization_x + eta * balance_x, float(num_train * batch_size))\n",
    "\n",
    "    logloss_y = -tf.reduce_sum(tf.multiply(ph['S_y'], theta_y) - tf.log(1.0 + tf.exp(theta_y)))\n",
    "    quantization_y = tf.reduce_sum(tf.pow((ph['b_batch'] - cur_g_batch), 2))\n",
    "    balance_y = tf.reduce_sum(tf.pow(tf.matmul(cur_g_batch, ph['ones_batch']) + tf.matmul(ph['G_'], ph['ones_']), 2))\n",
    "    loss_y = tf.div(logloss_y + gamma * quantization_y + eta * balance_y, float(num_train * batch_size))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(ph['lr'])\n",
    "\n",
    "    gradient_x = optimizer.compute_gradients(loss_x)\n",
    "    gradient_y = optimizer.compute_gradients(loss_y)\n",
    "    train_step_x = optimizer.apply_gradients(gradient_x)\n",
    "    train_step_y = optimizer.apply_gradients(gradient_y)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    loss_ = calc_loss(var['B'], var['F'], var['G'], Sim, gamma, eta)\n",
    "    print('...epoch: %3d, loss: %3.3f' % (0, loss_))\n",
    "    result = {}\n",
    "    result['loss'] = []\n",
    "    result['imapi2t'] = []\n",
    "    result['imapt2i'] = []\n",
    "\n",
    "    print('...training procedure starts')\n",
    "\n",
    "    for epoch in range(MAX_ITER):\n",
    "        lr = var['lr'][epoch]\n",
    "        # update F\n",
    "        var['F'] = train_img_net(image_input, cur_f_batch, var, ph,  train_x, train_L, lr, train_step_x, mean_pixel_, Sim)\n",
    "\n",
    "        # update G\n",
    "        var['G'] = train_txt_net(text_input, cur_g_batch, var, ph, train_y, train_L, lr, train_step_y, Sim)\n",
    "\n",
    "        # update B\n",
    "        var['B'] = np.sign(gamma * (var['F'] + var['G']))\n",
    "\n",
    "        # calculate loss\n",
    "        loss_ = calc_loss(var['B'], var['F'], var['G'], Sim, gamma, eta)\n",
    "        print('...epoch: %3d, loss: %3.3f, comment: update B' % (epoch + 1, loss_))\n",
    "\n",
    "        result['loss'].append(loss_)\n",
    "    print('...training procedure finish')\n",
    "    qBX = generate_image_code(image_input, cur_f_batch, query_x, bit, _meanpix)\n",
    "            #print(qBX)\n",
    "    qBY = generate_text_code(text_input, cur_g_batch, query_y, bit)\n",
    "            #print(qBY)\n",
    "    rBX = generate_image_code(image_input, cur_f_batch, retrieval_x, bit, _meanpix)\n",
    "            #print(rBX)\n",
    "    rBY = generate_text_code(text_input, cur_g_batch, retrieval_y, bit)\n",
    "            #print(rBY)\n",
    "\n",
    "    print(\"generated image and text codes\")\n",
    "\n",
    "    mapi2t = calc_map(qBX, rBY, query_L, retrieval_L)\n",
    "    mapt2i = calc_map(qBY, rBX, query_L, retrieval_L)\n",
    "\n",
    "    print('...test map: map(i->t): %3.3f, map(t->i): %3.3f' % (mapi2t, mapt2i))\n",
    "\n",
    "    result['mapi2t'] = mapi2t\n",
    "    result['mapt2i'] = mapt2i\n",
    "    result['lr'] = lr\n",
    "\n",
    "    fp = open(filename, 'wb')\n",
    "    pickle.dump(result, fp)\n",
    "\n",
    "    fp.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2 (tfenv)",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
